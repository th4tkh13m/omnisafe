# Copyright 2023 OmniSafe Team. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

defaults:
  # seed for random number generator
  seed: 0
  # training configurations
  train_cfgs:
    # device to use for training, options: cpu, cuda, cuda:0, cuda:0,1, etc.
    device: cpu
    # number of threads for torch
    torch_threads: 16
    # number of vectorized environments
    vector_env_nums: 1
    # number of parallel agent, similar to a3c
    parallel: 1
    # total number of steps to train
    total_steps: 1000000
    # number of evaluate episodes
    eval_episodes: 1
  # algorithm configurations
  algo_cfgs:
    # number of steps to update the policy
    steps_per_epoch: 2000
    # number of steps per sample
    update_cycle: 1
    # number of iterations to update the policy
    update_iters: 1
    # The size of replay buffer
    size: 1000000
    # The size of batch
    batch_size: 256
    # normalize reward
    reward_normalize: False
    # normalize cost
    cost_normalize: False
    # normalize observation
    obs_normalize: False
    # max gradient norm
    max_grad_norm: 40
    # use critic norm
    use_critic_norm: False
    # critic norm coefficient
    critic_norm_coeff: 0.001
    # The soft update coefficient
    polyak: 0.005
    # The discount factor of GAE
    gamma: 0.99
    # Actor perdorm random action before `start_learning_steps` steps
    start_learning_steps: 25000
    # The delay step of policy update
    policy_delay: 1
    # Whether to use the exploration noise
    use_exploration_noise: True
    # The exploration noise
    exploration_noise: 0.1
    # use cost
    use_cost: False
    batch_size: 256
    buffer_size: 1000000
    discount: 0.99
    target_update_freq: 250

    # Exploration
    buffer_size_before_training: 10000
    exploration_noise: 0.2

    # TD3
    target_policy_noise: 0.2
    noise_clip: 0.3

    # Encoder Loss
    dyn_weight: 1
    reward_weight: 0.1
    done_weight: 0.1

    # Replay Buffer (LAP)
    prioritized: True
    alpha: 0.4
    min_priority: 1
    enc_horizon: 5
    Q_horizon: 3

    # Encoder Model
    zs_dim: 512
    zsa_dim: 512
    za_dim: 256
    enc_hdim: 512
    enc_activ: 'elu'
    enc_lr: 0.0001
    enc_wd: 0.0001
    pixel_augs: True

    # Value Model
    value_hdim: 512
    value_activ: 'elu'
    value_lr: 0.0003
    value_wd: 0.0001
    value_grad_clip: 20

    # Policy Model
    policy_hdim: 512
    policy_activ: 'relu'
    policy_lr: 0.0003
    policy_wd: 0.0001
    gumbel_tau: 10
    pre_activ_weight: 0.00001

    # Reward model
    num_bins: 65
    lower: -10
    upper: 10
  # logger configurations
  logger_cfgs:
    # use wandb for logging
    use_wandb: False
    # wandb project name
    wandb_project: omnisafe
    # use tensorboard for logging
    use_tensorboard: True
    # save model frequency
    save_model_freq: 100
    # save logger path
    log_dir: "./runs"
    # save model path
    window_lens: 50
  # model configurations
  # model_cfgs:
  #   # weight initialization mode
  #   weight_initialization_mode: "kaiming_uniform"
  #   # actor type
  #   actor_type: mlp
  #   # linear learning rate decay
  #   linear_lr_decay: False
  #   # Configuration of Actor network
  #   actor:
  #     # Size of hidden layers
  #     hidden_sizes: [256, 256]
  #     # Activation function
  #     activation: relu
  #     # The learning rate of Actor network
  #     lr: 0.0003
  #   # Configuration of Critic network
  #   critic:
  #     # The number of critic networks
  #     num_critics: 1
  #     # Size of hidden layers
  #     hidden_sizes: [256, 256]
  #     # Activation function
  #     activation: relu
  #     # The learning rate of Critic network
  #     lr: 0.0003

# SafetyCarCircle1-v0:
#   # model configurations
#   model_cfgs:
#     # Configuration of Actor network
#     actor:
#       # The learning rate of Actor network
#       lr: 0.000005
#     # Configuration of Critic network
#     critic:
#       # The learning rate of Critic network
#       lr: 0.001

# SafetyCarGoal1-v0:
#   # model configurations
#   model_cfgs:
#     # Configuration of Actor network
#     actor:
#       # The learning rate of Actor network
#       lr: 0.000005
#     # Configuration of Critic network
#     critic:
#       # The learning rate of Critic network
#       lr: 0.001

# SafetyPointCircle1-v0:
#   # model configurations
#   model_cfgs:
#     # Configuration of Actor network
#     actor:
#       # The learning rate of Actor network
#       lr: 0.000005
#     # Configuration of Critic network
#     critic:
#       # The learning rate of Critic network
#       lr: 0.001

# SafetyPointGoal1-v0:
#   # model configurations
#   model_cfgs:
#     # Configuration of Actor network
#     actor:
#       # The learning rate of Actor network
#       lr: 0.000005
#     # Configuration of Critic network
#     critic:
#       # The learning rate of Critic network
#       lr: 0.001
